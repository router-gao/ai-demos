# ipynb file go through

For the first time user, copy each code block to Colab, and execute those code block step by step.

Get OS information (optional)

**get OS information**

```shell
!cat /etc/os-release
!cat /etc/hostname
!nvidia-smi
!nvidia-smi -L
!nvidia-smi -q
!python --version
```

**output**

```shell
PRETTY_NAME="Ubuntu 22.04.2 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.2 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
7efafce0d4db
Sun Sep 17 14:12:13 2023      
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |
| N/A   30C    P0    43W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                                
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-db44982c-3ffe-7915-4602-d81787eec492)
 
==============NVSMI LOG==============
 
Timestamp                                 : Sun Sep 17 14:12:13 2023
Driver Version                            : 525.105.17
CUDA Version                              : 12.0
 
Attached GPUs                             : 1
GPU 00000000:00:04.0
    Product Name                          : NVIDIA A100-SXM4-40GB
    Product Brand                         : NVIDIA
    Product Architecture                  : Ampere
    Display Mode                          : Enabled
    Display Active                        : Disabled
    Persistence Mode                      : Disabled
    MIG Mode
        Current                           : Disabled
        Pending                           : Disabled
    Accounting Mode                       : Disabled
    Accounting Mode Buffer Size           : 4000
    Driver Model
        Current                           : N/A
        Pending                           : N/A
    Serial Number                         : 1324321001924
    GPU UUID                              : GPU-db44982c-3ffe-7915-4602-d81787eec492
    Minor Number                          : 0
    VBIOS Version                         : 92.00.45.00.03
    MultiGPU Board                        : No
    Board ID                              : 0x4
    Board Part Number                     : 692-2G506-0200-003
    GPU Part Number                       : 20B0-884-A1
    Module ID                             : 6
    Inforom Version
        Image Version                     : G506.0200.00.04
        OEM Object                        : 2.0
        ECC Object                        : 6.16
        Power Management Object           : N/A
    GPU Operation Mode
        Current                           : N/A
        Pending                           : N/A
    GSP Firmware Version                  : N/A
    GPU Virtualization Mode
        Virtualization Mode               : Pass-Through
        Host VGPU Mode                    : N/A
    IBMNPU
        Relaxed Ordering Mode             : N/A
    PCI
        Bus                               : 0x00
        Device                            : 0x04
        Domain                            : 0x0000
        Device Id                         : 0x20B010DE
        Bus Id                            : 00000000:00:04.0
        Sub System Id                     : 0x134F10DE
        GPU Link Info
            PCIe Generation
                Max                       : 4
                Current                   : 4
                Device Current            : 4
                Device Max                : 4
                Host Max                  : N/A
            Link Width
                Max                       : 16x
                Current                   : 16x
        Bridge Chip
            Type                          : N/A
            Firmware                      : N/A
        Replays Since Reset               : 0
        Replay Number Rollovers           : 0
        Tx Throughput                     : 0 KB/s
        Rx Throughput                     : 0 KB/s
        Atomic Caps Inbound               : N/A
        Atomic Caps Outbound              : N/A
    Fan Speed                             : N/A
    Performance State                     : P0
    Clocks Throttle Reasons
        Idle                              : Active
        Applications Clocks Setting       : Not Active
        SW Power Cap                      : Not Active
        HW Slowdown                       : Not Active
            HW Thermal Slowdown           : Not Active
            HW Power Brake Slowdown       : Not Active
        Sync Boost                        : Not Active
        SW Thermal Slowdown               : Not Active
        Display Clock Setting             : Not Active
    FB Memory Usage
        Total                             : 40960 MiB
        Reserved                          : 446 MiB
        Used                              : 0 MiB
        Free                              : 40513 MiB
    BAR1 Memory Usage
        Total                             : 65536 MiB
        Used                              : 1 MiB
        Free                              : 65535 MiB
    Compute Mode                          : Default
    Utilization
        Gpu                               : 0 %
        Memory                            : 0 %
        Encoder                           : 0 %
        Decoder                           : 0 %
    Encoder Stats
        Active Sessions                   : 0
        Average FPS                       : 0
        Average Latency                   : 0
    FBC Stats
        Active Sessions                   : 0
        Average FPS                       : 0
        Average Latency                   : 0
    Ecc Mode
        Current                           : Enabled
        Pending                           : Enabled
    ECC Errors
        Volatile
            SRAM Correctable              : 0
            SRAM Uncorrectable            : 0
            DRAM Correctable              : 0
            DRAM Uncorrectable            : 0
        Aggregate
            SRAM Correctable              : 0
            SRAM Uncorrectable            : 0
            DRAM Correctable              : 0
            DRAM Uncorrectable            : 0
    Retired Pages
        Single Bit ECC                    : N/A
        Double Bit ECC                    : N/A
        Pending Page Blacklist            : N/A
    Remapped Rows
        Correctable Error                 : 0
        Uncorrectable Error               : 0
        Pending                           : No
        Remapping Failure Occurred        : No
        Bank Remap Availability Histogram
            Max                           : 640 bank(s)
            High                          : 0 bank(s)
            Partial                       : 0 bank(s)
            Low                           : 0 bank(s)
            None                          : 0 bank(s)
    Temperature
        GPU Current Temp                  : 30 C
        GPU T.Limit Temp                  : N/A
        GPU Shutdown Temp                 : 92 C
        GPU Slowdown Temp                 : 89 C
        GPU Max Operating Temp            : 85 C
        GPU Target Temperature            : N/A
        Memory Current Temp               : 42 C
        Memory Max Operating Temp         : 95 C
    Power Readings
        Power Management                  : Supported
        Power Draw                        : 43.62 W
        Power Limit                       : 400.00 W
        Default Power Limit               : 400.00 W
        Enforced Power Limit              : 400.00 W
        Min Power Limit                   : 100.00 W
        Max Power Limit                   : 400.00 W
    Clocks
        Graphics                          : 210 MHz
        SM                                : 210 MHz
        Memory                            : 1215 MHz
        Video                             : 585 MHz
    Applications Clocks
        Graphics                          : 1095 MHz
        Memory                            : 1215 MHz
    Default Applications Clocks
        Graphics                          : 1095 MHz
        Memory                            : 1215 MHz
    Deferred Clocks
        Memory                            : N/A
    Max Clocks
        Graphics                          : 1410 MHz
        SM                                : 1410 MHz
        Memory                            : 1215 MHz
        Video                             : 1290 MHz
    Max Customer Boost Clocks
        Graphics                          : 1410 MHz
    Clock Policy
        Auto Boost                        : N/A
        Auto Boost Default                : N/A
    Voltage
        Graphics                          : 712.500 mV
    Fabric
        State                             : N/A
        Status                            : N/A
    Processes                             : None
 
Python 3.10.12
```



mount Google drive to the file system (optional)

**mount Google drive to the file system (optional)**

```shell
#mount google drive
from google.colab import drive
drive.mount('/content/drive')
```

**output**

```shell
Mounted at /content/drive
```



import locale (highly suggested to do it, https://docs.python.org/3/library/locale.html)

**import locale**

```shell
import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding
```

**output**

```shell
null
```



git clone and download llama-2-7B to Colab

**git clone and download llama-2-7B to Colab**

```shell
!git clone https://github.com/facebookresearch/llama.git
!cd /content/llama && bash download.sh
```

![img](./21.2 - Lllama 2 Fine-tuning Demo - Colab, Fine-tuning ipynb file Go Through - HoTT Team - Public - VMware Core Confluence_files/image-2023-9-17_22-41-44.png)



Create directory and move files

**create directory and move files**

```shell
!mkdir /content/llama/llama-2-7b/7B/
!mv /content/llama/llama-2-7b/*.* /content/llama/llama-2-7b/7B
!mv /content/llama/token*.* /content/llama/llama-2-7b/
```



![img](./21.2 - Lllama 2 Fine-tuning Demo - Colab, Fine-tuning ipynb file Go Through - HoTT Team - Public - VMware Core Confluence_files/image-2023-9-17_22-43-35.png)

Download convert script and install transformers, accelerate, and sentencepiece 

Confluence page does not allow to post the script. Just post the screen shot as below.

![img](./21.2 - Lllama 2 Fine-tuning Demo - Colab, Fine-tuning ipynb file Go Through - HoTT Team - Public - VMware Core Confluence_files/image-2023-9-17_22-58-57.png)



Convert the Github Llama-2 model into Hugging Face (HF) format

**Convert the Github Llama-2 model into Hugging Face (HF) format**

```shell
!python convert_llama_weights_to_hf.py \
--input_dir /content/llama/llama-2-7b --model_size 7B --output_dir /content/llama/models_hf/7B
```

![img](./21.2 - Lllama 2 Fine-tuning Demo - Colab, Fine-tuning ipynb file Go Through - HoTT Team - Public - VMware Core Confluence_files/image-2023-9-17_23-16-22.png)

Install llama-recipes

**Install llama-recipes**

```shell
!pip install --extra-index-url https://download.pytorch.org/whl/test/cu118 llama-recipes
```

**output**

```shell
Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/test/cu118
Collecting llama-recipes
  Downloading llama_recipes-0.0.1-py3-none-any.whl (43 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 kB 1.2 MB/s eta 0:00:00
Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from llama-recipes) (0.23.0)
Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from llama-recipes) (1.4.4)
Collecting bitsandbytes (from llama-recipes)
  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.6/92.6 MB 19.6 MB/s eta 0:00:00
Collecting black (from llama-recipes)
  Downloading black-23.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 79.6 MB/s eta 0:00:00
Collecting datasets (from llama-recipes)
  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.6/519.6 kB 40.4 MB/s eta 0:00:00
Collecting fire (from llama-recipes)
  Downloading fire-0.5.0.tar.gz (88 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 12.3 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
Collecting loralib (from llama-recipes)
  Downloading loralib-0.1.2-py3-none-any.whl (10 kB)
Collecting optimum (from llama-recipes)
  Downloading optimum-1.13.1-py3-none-any.whl (396 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 396.1/396.1 kB 37.6 MB/s eta 0:00:00
Collecting peft (from llama-recipes)
  Downloading peft-0.5.0-py3-none-any.whl (85 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.6/85.6 kB 9.1 MB/s eta 0:00:00
Collecting py7zr (from llama-recipes)
  Downloading py7zr-0.20.6-py3-none-any.whl (66 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.7/66.7 kB 8.1 MB/s eta 0:00:00
Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llama-recipes) (1.11.2)
Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llama-recipes) (0.1.99)
Requirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-recipes) (2.0.1+cu118)
Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-recipes) (4.34.0.dev0)
Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->llama-recipes) (3.12.2)
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->llama-recipes) (4.5.0)
Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->llama-recipes) (1.12)
Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->llama-recipes) (3.1)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->llama-recipes) (3.1.2)
Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->llama-recipes) (2.0.0)
Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=2.0.1->llama-recipes) (3.27.4.1)
Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=2.0.1->llama-recipes) (16.0.6)
Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->llama-recipes) (0.17.1)
Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->llama-recipes) (1.23.5)
Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->llama-recipes) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->llama-recipes) (6.0.1)
Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->llama-recipes) (2023.6.3)
Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->llama-recipes) (2.31.0)
Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->llama-recipes) (0.13.3)
Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->llama-recipes) (0.3.3)
Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->llama-recipes) (4.66.1)
Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->llama-recipes) (5.9.5)
Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->llama-recipes) (8.1.7)
Collecting mypy-extensions>=0.4.3 (from black->llama-recipes)
  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)
Collecting pathspec>=0.9.0 (from black->llama-recipes)
  Downloading pathspec-0.11.2-py3-none-any.whl (29 kB)
Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->llama-recipes) (3.10.0)
Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->llama-recipes) (2.0.1)
Requirement already satisfied: ipython>=7.8.0 in /usr/local/lib/python3.10/dist-packages (from black->llama-recipes) (7.34.0)
Collecting tokenize-rt>=3.2.0 (from black->llama-recipes)
  Downloading tokenize_rt-5.2.0-py2.py3-none-any.whl (5.8 kB)
Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->llama-recipes) (9.0.0)
Collecting dill<0.3.8,>=0.3.0 (from datasets->llama-recipes)
  Downloading dill-0.3.7-py3-none-any.whl (115 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 13.9 MB/s eta 0:00:00
Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->llama-recipes) (1.5.3)
Collecting xxhash (from datasets->llama-recipes)
  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 21.7 MB/s eta 0:00:00
Collecting multiprocess (from datasets->llama-recipes)
  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 16.3 MB/s eta 0:00:00
Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->llama-recipes) (2023.6.0)
Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->llama-recipes) (3.8.5)
Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llama-recipes) (1.16.0)
Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llama-recipes) (2.3.0)
Collecting coloredlogs (from optimum->llama-recipes)
  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 5.4 MB/s eta 0:00:00
Collecting texttable (from py7zr->llama-recipes)
  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)
Collecting pycryptodomex>=3.6.6 (from py7zr->llama-recipes)
  Downloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 90.2 MB/s eta 0:00:00
Collecting pyzstd>=0.14.4 (from py7zr->llama-recipes)
  Downloading pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 412.3/412.3 kB 40.0 MB/s eta 0:00:00
Collecting pyppmd<1.1.0,>=0.18.1 (from py7zr->llama-recipes)
  Downloading pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.8/138.8 kB 14.7 MB/s eta 0:00:00
Collecting pybcj>=0.6.0 (from py7zr->llama-recipes)
  Downloading pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.8/49.8 kB 6.3 MB/s eta 0:00:00
Collecting multivolumefile>=0.2.3 (from py7zr->llama-recipes)
  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)
Collecting brotli>=1.0.9 (from py7zr->llama-recipes)
  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 85.8 MB/s eta 0:00:00
Collecting inflate64>=0.3.1 (from py7zr->llama-recipes)
  Downloading inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.1/93.1 kB 12.4 MB/s eta 0:00:00
Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->llama-recipes) (23.1.0)
Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->llama-recipes) (3.2.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->llama-recipes) (6.0.4)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->llama-recipes) (4.0.3)
Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->llama-recipes) (1.9.2)
Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->llama-recipes) (1.4.0)
Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->llama-recipes) (1.3.1)
Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black->llama-recipes) (67.7.2)
Collecting jedi>=0.16 (from ipython>=7.8.0->black->llama-recipes)
  Downloading jedi-0.19.0-py2.py3-none-any.whl (1.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 75.9 MB/s eta 0:00:00
Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black->llama-recipes) (4.4.2)
Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black->llama-recipes) (0.7.5)
Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black->llama-recipes) (5.7.1)
Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black->llama-recipes) (3.0.39)
Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black->llama-recipes) (2.16.1)
Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black->llama-recipes) (0.2.0)
Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black->llama-recipes) (0.1.6)
Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black->llama-recipes) (4.8.0)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->llama-recipes) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->llama-recipes) (2.0.4)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->llama-recipes) (2023.7.22)
Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->llama-recipes) (3.20.3)
Collecting humanfriendly>=9.1 (from coloredlogs->optimum->llama-recipes)
  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 12.2 MB/s eta 0:00:00
Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.1->llama-recipes) (2.1.3)
Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->llama-recipes) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->llama-recipes) (2023.3.post1)
Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.1->llama-recipes) (1.3.0)
Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.8.0->black->llama-recipes) (0.8.3)
Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.8.0->black->llama-recipes) (0.7.0)
Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.8.0->black->llama-recipes) (0.2.6)
Building wheels for collected packages: fire
  Building wheel for fire (setup.py) ... done
  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=e1c965b04226df022bea89ea4afa70277bdadc2e04757a00117ffeb4006bc005
  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95
Successfully built fire
Installing collected packages: texttable, brotli, bitsandbytes, xxhash, tokenize-rt, pyzstd, pyppmd, pycryptodomex, pybcj, pathspec, mypy-extensions, multivolumefile, loralib, jedi, inflate64, humanfriendly, fire, dill, py7zr, multiprocess, coloredlogs, black, datasets, peft, optimum, llama-recipes
Successfully installed bitsandbytes-0.41.1 black-23.9.1 brotli-1.1.0 coloredlogs-15.0.1 datasets-2.14.5 dill-0.3.7 fire-0.5.0 humanfriendly-10.0 inflate64-0.3.1 jedi-0.19.0 llama-recipes-0.0.1 loralib-0.1.2 multiprocess-0.70.15 multivolumefile-0.2.3 mypy-extensions-1.0.0 optimum-1.13.1 pathspec-0.11.2 peft-0.5.0 py7zr-0.20.6 pybcj-1.0.1 pycryptodomex-3.19.0 pyppmd-1.0.0 pyzstd-0.15.9 texttable-1.6.7 tokenize-rt-5.2.0 xxhash-3.3.0

```



Load and test the original model

**load and test the original model**

```shell
#load and test the original model
import torch
from transformers import LlamaForCausalLM, LlamaTokenizer
model_id="/content/llama/models_hf/7B"
tokenizer = LlamaTokenizer.from_pretrained(model_id)
model =LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)
 
eval_prompt = """
Summarize this dialog:
A: Hi Tom, are you busy tomorrow’s afternoon?
B: I’m pretty sure I am. What’s up?
A: Can you go with me to the animal shelter?.
B: What do you want to do?
A: I want to get a puppy for my son.
B: That will make him so happy.
A: Yeah, we’ve discussed it many times. I think he’s ready now.
B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)
A: I'll get him one of those little dogs.
B: One that won't grow up too big;-)
A: And eat too much;-))
B: Do you know which one he would like?
A: Oh, yes, I took him there last Monday. He showed me one that he really liked.
B: I bet you had to drag him away.
A: He wanted to take it home right away ;-).
B: I wonder what he'll name it.
A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
---
Summary:
"""
 
model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")
 
model.eval()
with torch.no_grad():
    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))
```



Output is as below. The Summary is not a perfect one. It looks like a copy/paste from the dialog.

GPU usage is 8.7/40.0 GB

![img](./21.2 - Lllama 2 Fine-tuning Demo - Colab, Fine-tuning ipynb file Go Through - HoTT Team - Public - VMware Core Confluence_files/image-2023-9-17_23-20-16.png)



Fine-tune the llama-2-7B model

**Fine-tune the llama-2-7B model**

```shell
!export CUDA_VISIBLE_DEVICES=0
!python -m llama_recipes.finetuning --report_to wandb --use_peft --peft_method lora --quantization --model_name /content/llama/models_hf/7B --output_dir /content/llama/PEFT/model
```

output

![img](./21.2 - Lllama 2 Fine-tuning Demo - Colab, Fine-tuning ipynb file Go Through - HoTT Team - Public - VMware Core Confluence_files/image-2023-9-18_0-9-35.png)

![img](./21.2 - Lllama 2 Fine-tuning Demo - Colab, Fine-tuning ipynb file Go Through - HoTT Team - Public - VMware Core Confluence_files/image-2023-9-18_10-29-52.png)

Test the tuned model

**test the tuned model**

```shell
#test the tuned model
import torch
from transformers import LlamaForCausalLM, LlamaTokenizer
from peft import PeftModel, PeftConfig
 
model_id="/content/llama/models_hf/7B"
tokenizer = LlamaTokenizer.from_pretrained(model_id)
model = LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)
model = PeftModel.from_pretrained(model, "/content/llama/PEFT/model")
 
eval_prompt = """
Summarize this dialog:
A: Hi Tom, are you busy tomorrow’s afternoon?
B: I’m pretty sure I am. What’s up?
A: Can you go with me to the animal shelter?.
B: What do you want to do?
A: I want to get a puppy for my son.
B: That will make him so happy.
A: Yeah, we’ve discussed it many times. I think he’s ready now.
B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)
A: I'll get him one of those little dogs.
B: One that won't grow up too big;-)
A: And eat too much;-))
B: Do you know which one he would like?
A: Oh, yes, I took him there last Monday. He showed me one that he really liked.
B: I bet you had to drag him away.
A: He wanted to take it home right away ;-).
B: I wonder what he'll name it.
A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))
---
Summary:
"""
 
model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")
 
model.eval()
with torch.no_grad():
    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))
```

Output, the fine-tuned model can generate the summary of the dialog.

![img](./21.2 - Lllama 2 Fine-tuning Demo - Colab, Fine-tuning ipynb file Go Through - HoTT Team - Public - VMware Core Confluence_files/image-2023-9-18_10-31-38.png)